{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vBGUEY1VGYG"
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = [] # preprocessed data\n",
    "path2 = [] # annotations\n",
    "\n",
    "num_data_max = 5 # data의 개수 (SN001~SN005일 경우 : 5)\n",
    "\n",
    "for i in range (1,num_data_max+1):\n",
    "    idx = str(i).zfill(3)\n",
    "    path1.append(\"./pre_SN\"+idx+\".edf\")\n",
    "    \n",
    "for i in range (1,num_data_max+1):\n",
    "    idx = str(i).zfill(3)\n",
    "    path2.append(\"./SN\"+idx+\"_sleepscoring.edf\")\n",
    "\n",
    "pre = []\n",
    "for idx in path1:\n",
    "    pre.append(mne.io.read_raw_edf(idx, preload=True))\n",
    "#     편의에 따라\n",
    "#     pre.append(os.path.join(\"./\", \"SN\" + idx + \".edf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qehmZeWEiPF6"
   },
   "outputs": [],
   "source": [
    "# 채널 타입을 설정하기 위한 사전 정의\n",
    "channel_types = {\n",
    "    'EEG F4-M1': 'eeg',  # 인덱스 0\n",
    "    'EEG C4-M1': 'eeg',  # 인덱스 1\n",
    "    'EEG O2-M1': 'eeg',  # 인덱스 2\n",
    "    'EEG C3-M2': 'eeg',  # 인덱스 3\n",
    "    'EMG chin': 'emg',  # 인덱스 4, EMG chin\n",
    "    'EOG E1-M2': 'eog',  # 인덱스 5, EOG\n",
    "    'EOG E2-M2': 'eog',  # 인덱스 6, EOG\n",
    "    'ECG': 'ecg'   # 인덱스 7, ECG\n",
    "}\n",
    "\n",
    "# 채널 타입을 설정\n",
    "for i in pre:\n",
    "    i.set_channel_types(channel_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9mjQURzlfNZ"
   },
   "source": [
    "# 수면단계와 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoHXa1k2lZIs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 어노테이션 데이터 로드\n",
    "annotations = []\n",
    "for idx in path2:\n",
    "    annotations.append(mne.read_annotations(idx))\n",
    "\n",
    "# pre 데이터 객체에 어노테이션 설정\n",
    "for i in range(0,len(annotations)):\n",
    "    pre[i].set_annotations(annotations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyHgM_O-aKge"
   },
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTx9cSecaKgf"
   },
   "outputs": [],
   "source": [
    "def normalize_raw_data(raw, channel_types={'eeg': True, 'eog': True, 'emg': True, 'ecg': True}):\n",
    "\n",
    "    # Ensure the data is preloaded\n",
    "    if not raw.preload:\n",
    "        raw.load_data()\n",
    "\n",
    "    # Pick specified channel types\n",
    "    channel_indices = mne.pick_types(raw.info, **channel_types)\n",
    "\n",
    "    # Initialize the standard scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Retrieve the data for selected channels\n",
    "    data = raw.get_data(picks=channel_indices)\n",
    "\n",
    "    # Reshape data for scaling\n",
    "    data = data.reshape(data.shape[0], -1).T  # Transpose to have features along rows as expected by StandardScaler\n",
    "\n",
    "    # Fit and transform the data\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    # Replace original data with scaled data\n",
    "    raw._data[channel_indices, :] = scaled_data.T  # Transpose back to original shape\n",
    "\n",
    "    return raw\n",
    "\n",
    "# Normalize all_pre directly\n",
    "for i in pre:\n",
    "    i = normalize_raw_data(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWKWBW86aKgf"
   },
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YwBN0hHaKgf"
   },
   "source": [
    "## Annotation setting\n",
    "\n",
    "- Lights off, Lights on 제거해서 기존 856개 -> 854개 annotations(labels) : 1번째 data(856), 2번째 data(858), 3번째 data(956)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJYA3-KLaKgf"
   },
   "outputs": [],
   "source": [
    "def annotation_remove(annotations):\n",
    "    # Find Lights off & Lights on\n",
    "    indices_to_remove = [idx for idx, desc in enumerate(annotations.description) if desc.startswith('Lights off') or desc.startswith('Lights on')]\n",
    "\n",
    "    # Create new annotations\n",
    "    new_annotations = mne.Annotations(onset=[annotations.onset[i] for i in range(len(annotations.onset)) if i not in indices_to_remove],\n",
    "                                      duration=[annotations.duration[i] for i in range(len(annotations.duration)) if i not in indices_to_remove],\n",
    "                                      description=[annotations.description[i] for i in range(len(annotations.description)) if i not in indices_to_remove],\n",
    "                                      orig_time=annotations.orig_time)\n",
    "    return new_annotations\n",
    "\n",
    "for i in range(len(annotations)):\n",
    "    annotations[i] = annotation_remove(annotations[i])\n",
    "\n",
    "# check removal of two annotations\n",
    "# labels = np.array(annotations[2].description)\n",
    "# print(\"Label shape:\", labels.shape)\n",
    "# print(\"Unique label values:\", np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zk7hCskaKgg"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qz46eLyWaKgg",
    "outputId": "b7776ec9-6905-4079-c806-bb241397aaa9"
   },
   "outputs": [],
   "source": [
    "all_possible_labels = ['Sleep stage W','Sleep stage R','Sleep stage N1','Sleep stage N2','Sleep stage N3',]\n",
    "\n",
    "label_encoder = LabelEncoder().fit(all_possible_labels)\n",
    "\n",
    "# initialize list for encoded labels\n",
    "encoded_labels = []\n",
    "\n",
    "for idx in annotations:\n",
    "    encoded_labels.append(label_encoder.transform(np.array(idx.description)))\n",
    "\n",
    "for idx in encoded_labels:\n",
    "    print(idx.shape)\n",
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWFmrbczdW3S",
    "outputId": "3b9b5f08-1ccc-4b2f-e518-8add9d9216fc"
   },
   "outputs": [],
   "source": [
    "epoch_duration =30\n",
    "\n",
    "# Create events every 30 seconds\n",
    "events = []\n",
    "for idx in pre:\n",
    "    events.append(mne.make_fixed_length_events(idx, duration=epoch_duration))\n",
    "\n",
    "# Create the epochs\n",
    "epochs = []\n",
    "for i in range(0,len(pre)):\n",
    "    epochs.append(mne.Epochs(pre[i], events[i], tmin=0.0, tmax=epoch_duration - 1 / 100, baseline=None, preload=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hGmnCDJlrD4",
    "outputId": "98cbc924-1536-428e-e657-7f215d19540f"
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "num_epochs = 800  # 각 데이터의 epoch 개수\n",
    "\n",
    "for i in range(0,len(epochs)):\n",
    "    data_list.append(epochs[i].get_data())  #3D array (n epochs, 8 channels, 3000 timepoints)\n",
    "    data_list[i]= np.float32(data_list[i][:-1]) #이건 마지막 labeling 안된 에포크 제거하는 줄\n",
    "    #data_list[i]= np.float32(data_list[i][:num_epochs+i]) #kernel 자꾸 죽어서 크기 작게 slicing 해봄..\n",
    "\n",
    "labels_list = []\n",
    "for i in range(0,len(encoded_labels)):\n",
    "    labels_list.append(encoded_labels[i])\n",
    "    #labels_list.append(encoded_labels[i][:num_epochs+i])\n",
    "\n",
    "print(\"data shape : \", len(data_list), \"*\", data_list[0].shape)\n",
    "print(\"label shape : \", len(labels_list), \"*\",labels_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgHuqhVMn9gv"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for epochs, labels in dataloader:\n",
    "            epochs = epochs.squeeze(0).float()  # Remove batch dimension and convert to float\n",
    "            labels = labels.squeeze(0).long()  # Remove batch dimension and convert to long\n",
    "            outputs = model(epochs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q38LtibYhex5"
   },
   "outputs": [],
   "source": [
    "# Custom dataset for PSG data\n",
    "class PSGDataset(Dataset):\n",
    "    def __init__(self, data_list, labels_list):\n",
    "        self.data_list = data_list\n",
    "        self.labels_list = labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]  # shape: (n_epochs, 8, 3000)\n",
    "        labels = self.labels_list[idx]  # shape: (n_epochs,)\n",
    "        return torch.tensor(data, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "train_split = 11\n",
    "train_data_list = data_list[:train_split]\n",
    "train_labels_list = labels_list[:train_split]\n",
    "test_data_list = data_list[train_split:]\n",
    "test_labels_list = labels_list[train_split:]\n",
    "\n",
    "train_dataset = PSGDataset(train_data_list, train_labels_list)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = PSGDataset(test_data_list, test_labels_list)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSuOxAbzitYX"
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for epochs, _ in data_loader:\n",
    "            epochs = epochs.squeeze(0).float()  # Remove batch dimension and convert to float\n",
    "            outputs = model(epochs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    return all_predictions\n",
    "\n",
    "# Example usage:\n",
    "predictions = predict(model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "ZUyhqs_WnvUj",
    "outputId": "33a8cd91-3b02-4e9a-d414-b7f7d163b5a4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Sample input lists (assuming you have these prepared)\n",
    "data_list = [torch.randn(80, 8, 3000) for _ in range(10)]\n",
    "labels_list = [torch.randint(0, 5, (80,)) for _ in range(10)]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data_list = data_list[:7]\n",
    "train_labels_list = labels_list[:7]\n",
    "test_data_list = data_list[7:]\n",
    "test_labels_list = labels_list[7:]\n",
    "\n",
    "# Dataset, DataLoader definition\n",
    "class PSGDataset(Dataset):\n",
    "    def __init__(self, data_list, labels_list):\n",
    "        self.data_list = data_list\n",
    "        self.labels_list = labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(data) for data in self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cumulative_idx = 0\n",
    "        for i, data in enumerate(self.data_list):\n",
    "            if cumulative_idx + len(data) > idx:\n",
    "                item_idx = idx - cumulative_idx\n",
    "                return self.data_list[i][item_idx], self.labels_list[i][item_idx]\n",
    "            cumulative_idx += len(data)\n",
    "        raise IndexError(\"Index out of range\")\n",
    "\n",
    "train_dataset = PSGDataset(train_data_list, train_labels_list)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = PSGDataset(test_data_list, test_labels_list)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model definition\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_channels, seq_length, num_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_channels, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.lstm = nn.LSTM(input_size=32 * (seq_length // 4), hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, seq_length = x.size()\n",
    "        print(\"Initial shape:\", x.shape)\n",
    "\n",
    "        # Pass through first convolutional layer and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Shape: (batch_size, 16, seq_length/2)\n",
    "        print(\"After conv1 and pool:\", x.shape)\n",
    "\n",
    "        # Pass through second convolutional layer and pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Shape: (batch_size, 32, seq_length/4)\n",
    "        print(\"After conv2 and pool:\", x.shape)\n",
    "\n",
    "        # Reshape for LSTM layer\n",
    "        x = x.permute(0, 2, 1)  # Shape: (batch_size, seq_length/4, 32)\n",
    "        print(\"After permute:\", x.shape)\n",
    "\n",
    "        # LSTM layer\n",
    "        x, _ = self.lstm(x)  # Shape: (batch_size, seq_length/4, 128)\n",
    "        print(\"After LSTM:\", x.shape)\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x[:, -1, :])  # Shape: (batch_size, num_classes)\n",
    "        print(\"After FC:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CNN_LSTM(num_channels=8, seq_length=3000, num_classes=5).to(device)  # Move model to GPU\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data)\n",
    "\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Loss function and optimizer definition\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(dataloader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "writer = SummaryWriter('runs/experiment_1')\n",
    "\n",
    "# Model training\n",
    "num_epochs = 15  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate training loss\n",
    "    training_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = calculate_accuracy(train_dataloader, model)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = calculate_accuracy(test_dataloader, model)\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/train', training_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "    writer.add_scalar('Accuracy/test', test_accuracy, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {training_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DesJ5qKb17DZ"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
